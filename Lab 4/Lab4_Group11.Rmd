---
title: '732A90: Computational Statistics'
author: "Sofie Jörgensen, Oriol Garrobé Guilera, David Hrabovszki"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    fig_caption: yes
    includes:
      in_header: my_header.tex
subtitle: Computer lab4 - Group11
header-includes: \usepackage{float}
---

```{r setup, include=FALSE, eval=TRUE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Question 1: Computations with Metropolis-Hastings

In this question we define the density function $f(x)$ as:

$$f(x)\propto x^5e^{-x}, x>0$$
This density function is known up to some constant of proportionality, which is 120.

```{r warning=FALSE}
# R version
RNGversion('3.5.1')
#libraries
library(coda)
```


### 1.

In the first step we use the Metropolis-Hastings algorithm to generate samples from the posterior distribution $f(x)$ by using
the log-normal $LN(X_t,1)$ as the proposal distribution. We take 1 as a starting point, because we expect this value to be within the range of possible values of the distribution defined above.

```{r}

# 1.1.
posterior <- function(x){
  x^5 * exp(-x)
}

# Seed
set.seed(1234567890)

posterior_sampler_ln <- function(tmax=1000, starting_point = 1) {
  X=rep(starting_point,tmax)
  for (t in 1:tmax) {
    Y = rlnorm(1,meanlog = X[t],sdlog = 1)
    U = runif(1,min = 0, max = 1)
    alpha = min(1,
                (posterior(Y) * dlnorm(X[t],meanlog = Y, sdlog = 1)) /
                (posterior(X[t]) * dlnorm(Y, meanlog = X[t], sdlog = 1)))
    if (U < alpha) {
      X[t+1] = Y
    } else {
      X[t+1] = X[t]
    }
    t = t+1
  }
  return(X)
}
```


The time series plot of the obtained chain of samples can be observed in Figure \ref{fig:lognormal}. The plot does not show that the chain converges, because it often takes the same value for many iterations and then jumps to a very different one. This implies that the sample is not close to the posterior. There does not seem to be a burn-in period, because samples from many iterations are close to the starting point (which was 1 in this case).

```{r, fig.cap="\\label{fig:lognormal} Metropolis-Hastings sampler with log-normal proposal", out.width = "80%", fig.pos='h', fig.align='center' }
tmax=1000
# Seed
set.seed(1234567890)
ln_sample = posterior_sampler_ln(tmax = tmax, starting_point = 1)
plot(1:(tmax+1),ln_sample, type ='l', xlab = "time", ylab = "X")
```

```{r, fig.cap="\\label{fig:lognormal_hist} Histogram of generated samples with log-normal proposal (MH sampler)", out.width = "80%", fig.pos='h', fig.align='center' }

hist(ln_sample, breaks = 20, main = "", xlab = "X")
```


### 2.

Now we perform Step 1 using the chi-square distribution $\chi^2(\lfloor X_t+1\rfloor)$ as the proposal distribution, where $\lfloor x\rfloor$ is the \texttt{floor()} function.

```{r}

# 1.2
#Set seed
set.seed(1234567890)

posterior_sampler_chisq <- function(tmax=1000, starting_point = 1) {
  X=rep(starting_point,tmax)
  for (t in 1:tmax) {
    Y = rchisq(1,df = floor(X[t]+1))
    U = runif(1,min = 0, max = 1)
    alpha = min(1,
                (posterior(Y) * dchisq(X[t],df = floor(Y+1))) /
                (posterior(X[t]) * dchisq(Y, df = floor(X[t]+1))))
    if (U < alpha) {
      X[t+1] = Y
    } else {
      X[t+1] = X[t]
    }
    t = t+1
  }
  return(X)
}
```

The time series plot is shown in Figure \ref{fig:chisquare}, and it can be seen that the chain converges much more than in the previous case. Again there does not seem to be a burn-in period, the starting value of 1 seems to be a probable value of the distribution, so we do not discard any samples from the beginning of the chain.

```{r, fig.cap="\\label{fig:chisquare} Metropolis-Hastings sampler with chi-square proposal", out.width = "80%", fig.pos='h', fig.align='center' }
tmax=1000
# Seed
set.seed(1234567890)
chi_sample = posterior_sampler_chisq(tmax = tmax, starting_point = 1)
plot(1:(tmax+1), chi_sample, type = 'l', xlab = "time", ylab = "X")
```

```{r, fig.cap="\\label{fig:chisquare_hist} Histogram of generated samples with Chi-square proposal (MH sampler)", out.width = "80%", fig.pos='h', fig.align='center' }
hist(chi_sample, breaks = 20, main = "", xlab = "X")
```


### 3.

We recognise that the probability distribution function $f(x)$ belongs to a gamma distribution, therefore we know that $f(x)$ must have a positive skew. This attribute is also true for the distribution of samples from Step 2 (Figure \ref{fig:chisquare_hist}), but untrue for the distribution of samples from Step 1 (Figure \ref{fig:lognormal_hist}). Also, the chain with the log-normal proposal does not converge, but the chain with the Chi-square proposal does, so we conclude that the samples generated from Step 2 are a better approximation of samples from the posterior distribution.

### 4.

Now we generate 10 MCMC sequences using the sample generators from Step 2, and analyze their convergence using the Gelman-Rubin method. The starting points for the 10 sequences are 1,2,...,10.

The output of the Gelman-Rubin's convergence diagnostic for 1000 iterations:

```{r}

#1.4
starting_points = seq(1,10)
MCMC_seqs = mcmc.list()
tmax=1000
# Seed
set.seed(1234567890)
for (k in starting_points) {
  MCMC_seqs[[k]] = as.mcmc(posterior_sampler_chisq(tmax = tmax, starting_point = k))
}

print(gelman.diag(MCMC_seqs))
```

The potential scale reduction factor is based on the comparison of within-chain and between-chain variances. For 1000 iterations, its point estimate is around 1 and its upper confidence limit is also very close to 1, so according to this diagnostics, approximate convergence has been achieved at 1000 iterations. Even for only 100 iterations, both the point estimate and the upper limit are under 1.1.



### 5.

In this step we estimate

$$\int\limits_{0}^{\infty}xf(x)dx$$
using the samples generated from Steps 1 and 2.

Based on the slides from Lecture 4 we can use MCMC samples from $p(\theta|D)$ to obtain a point estimator by calculating the sample average: $$\theta^* = \int\theta p(\theta|D) \approx \frac{1}{n} \sum\limits_{i=1}^{n}\theta_i$$
```{r}

# 1.5
# Seed
set.seed(1234567890)
#using Step1
#mean(ln_sample) #large variance; 2.949404

#using Step2
#mean(chi_sample) #small variance; 5.636084
```

The sample average of the chain generated in Step 1 is **2.95** and in Step 2 is **5.63**.

If we do not set the seed before the sample generation, then we get highly variable averages from Step 1, but from Step 2 it is almost always between 5.5 and 6.5.

### 6.

According to Definition 4.5 in Mathematical Statistics with Applications by Wackerly et al., the integral in Step 5 is equal to the expected value of random variable X, where X can take values between 0 and $\infty$.

The generated distribution with probability density function $f(x)$ is a gamma distribution, so we can express $f(x)$ in its general form as
$$f(x) = \Bigg{[}\frac{1}{\Gamma(\alpha)\beta^\alpha}\Bigg{]}x^{\alpha-1}e^{-x/\beta} $$, where $0<x<\infty$

Thus $E(X) = \alpha\beta$. In our case $\alpha = 6, \beta=1$, so $E(X) = 6$.

This is the actual value of the integral in Step 5, so we conclude that our estimation using the samples generated in Step 2 with a Chi-square proposal distribution seems accurate, whereas the estimation from the samples of Step 1 with the log-normal proposal does not seem accurate at all.

----------

# Question 2: Gibbs sampling

### 1.

We import into R the data containing the concentration of a certain chemical in a water sample is found in \texttt{chemical.RData}, having the following variables:
 -\texttt{X}: day of the measurment
 -\texttt{Y}: measured concentration of the chemical
The code can be found in the Appendix. 

```{r}


############################## Question 2: Gibbs sampling ###############################

load(file = "chemical.RData") 
```

In \ref{fig:chemical} there is a plot of the dependence of \texttt{Y} on \texttt{X}.  

```{r, fig.cap="\\label{fig:chemical} Dependendence of measured concentration of the chemical on day of the measurment", out.width = "80%", fig.pos='h', fig.align='center' }
# 2.1

plot(X,Y, xlab = "Day of the measurment"
     , ylab = "Measured concentration of the chemical", col = "grey", pch = 16)
```

### 2.

Using the random-walk Bayesian model with $n$ number of observations and $\vec{\mu}=(\mu_{1},...,\mu_{n})$ uknown parameters: $$Y_{i}=N(\mu_{i}, 0.2),\quad i=1,...,n$$

We compute the likelihood $p(\vec{Y}|\vec{\mu})$ following the steps below:

\begin{eqnarray*}
p(\vec{Y}|\vec{\mu}) &=& \prod_{j=1}^{n}f(Y_{j}|\mu_{j},0.2) \\
                     &=& \prod_{j=1}^{n}\frac{1}{\sigma\sqrt{2\pi}}exp\left[-\frac{(Y_{j}-\mu_{j})^2}{2\sigma^{2}}\right]\\
                     &=& \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n exp\left[-\frac{\sum_{j=1}^{n}(Y_{j}-\mu_{j})^2}{2\sigma^{2}}\right]\\
\end{eqnarray*}

In order to compute the prior $p(\mu_{i})$ a chain rule is used. Being the chain rule:$p(\vec{\mu})=p(\mu_{1})p(\mu_{2}|\mu_{1})...p(\mu_{n}|\mu_{n-1})$ 

Let the prior be, 

\begin{equation}
p(\mu_{1})=1\\
p(\mu_{i+1|\mu_{i}})=N(\mu_{i}, 0.2)
\end{equation}

We follow the steps below:

\begin{eqnarray*}
p(\vec{\mu}) &=& \prod_{i=1}^{n}p(\mu_{1})p(\mu_{i+1}|\mu_{i}) \\
             &=& \prod_{i=2}^{n}N(\mu_{i},0.2)\\
             &=& \prod_{i=2}^{n}\frac{1}{\sigma\sqrt{2\pi}}exp\left[-\frac{(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]\\
             &=& \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n exp\left[-\frac{\sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]\\
\end{eqnarray*}

### 3.

In order to get the posterior up to a constant proportionality, we use the Bayes'. Therefore, 

\begin{eqnarray*}
p(\vec{\mu}|\vec{Y}) &\propto& p(\vec{Y}|\vec{\mu})p(\vec{\mu})\\
                     &\propto& \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n exp\left[-\frac{\sum_{j=1}^{n}(Y_{j}-\mu_{j})^2}{2\sigma^{2}}\right] \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n exp\left[-\frac{\sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]\\
                     &\propto& exp\left[-\frac{\sum_{j=1}^{n}(Y_{j}-\mu_{j})^2}{2\sigma^{2}}\right] exp\left[-\frac{\sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]\\
                     &\propto& exp\left[-\frac{\sum_{j=1}^{n}(Y_{j}-\mu_{j})^2 + \sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]
\end{eqnarray*}

And finally, once we have the posterior, we can compute the the probability distribution $p(\mu_{i}|\vec{\mu}_{i},\vec{Y})$, where $\vec\mu_{-i}$ is a vector containing all $\mu$ values except for $\mu_{i}$.We also know that we can discard all the elements that do not contain $\vec\mu_{i}$ as we are looking for a posterior up to a constant proportionality. Using Hint C, we get to the following result:

$$p(\mu_{i}|\vec{\mu}_{-i},\vec{Y}) \propto exp\left[-\frac{\left(\mu_{i}-(Y_{i}+Y_{i-1}+Y_{i+1})/3\right)^{2}}{2\sigma^{2}/3}\right]$$

### 4. 

We implement the Gibbs samples using the distributions found and initial value $\vec\mu^{0}=(0,...,0)$.  This code snippet (see Appendix) computes $n$ values of $\vec\mu$ and computes the expected value of $\vec\mu$ using a Monte Carlo approach. In this particular case $n=1000$. 

The expected value of $\vec\mu$ is: 

### I'M NOT SURE ABOUT THIS
```{r}
# 2.4

gibbs_sampler <- function(Tmax) {
  d <- length(Y)
  sigma <- 0.2
  mu <- matrix(0, nrow = Tmax, ncol = d)
  t <- 0
  while (t<Tmax) {
    for (i in 1:d) {
      mu[t+1,i] <- exp((-3*(mu[i]-(Y[i]+Y[i+1])/3)^2)/(2*sigma^2))
    }
    
  t=t+1#i added this, so it runs \David
  }
  
  
  mu
}

gibbs_sampler(10)
```


----------

# Appendix

```{r ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

```