---
title: '732A90: Computational Statistics'
author: "Sofie Jörgensen, Oriol Garrobé Guilera, David Hrabovszki"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    fig_caption: yes
    includes:
      in_header: my_header.tex
subtitle: Computer lab4 - Group11
header-includes: \usepackage{float}
---

```{r setup, include=FALSE, eval=TRUE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Question 1: Computations with Metropolis-Hastings

In this question we define the density function $f(x)$ as

$$f(x)\propto x^5e^{-x}, \quad x>0.$$
This density function is known up to some constant of proportionality, which is 120.

```{r warning=FALSE}
# R version
RNGversion('3.5.1')
#libraries
library(coda)
```


### 1.

In the first step we use the Metropolis-Hastings algorithm to generate samples from the posterior distribution $f(x)$ by using
the log-normal $LN(X_t,1)$ as the proposal distribution. We take 1 as a starting point, because we expect this value to be within the range of possible values of the distribution defined above.

```{r}

# 1.1.
posterior <- function(x){
  x^5 * exp(-x)
}

# Seed
set.seed(1234567890)

posterior_sampler_ln <- function(tmax=1000, starting_point = 1) {
  X=rep(starting_point,tmax)
  for (t in 1:tmax) {
    Y = rlnorm(1,meanlog = X[t],sdlog = 1)
    U = runif(1,min = 0, max = 1)
    alpha = min(1,
                (posterior(Y) * dlnorm(X[t],meanlog = Y, sdlog = 1)) /
                (posterior(X[t]) * dlnorm(Y, meanlog = X[t], sdlog = 1)))
    if (U < alpha) {
      X[t+1] = Y
    } else {
      X[t+1] = X[t]
    }
    t = t+1
  }
  return(X)
}
```


The time series plot of the obtained chain of samples can be observed in Figure \ref{fig:lognormal}. The plot does not show that the chain converges, because it often takes the same value for many iterations and then jumps to a very different one. This implies that the sample is not close to the posterior. There does not seem to be a burn-in period, because samples from many iterations are close to the starting point (which was 1 in this case).

```{r, fig.cap="\\label{fig:lognormal} Metropolis-Hastings sampler with log-normal proposal", out.width = "80%", fig.pos='h', fig.align='center' }
tmax=1000
# Seed
set.seed(1234567890)
ln_sample = posterior_sampler_ln(tmax = tmax, starting_point = 1)
plot(1:(tmax+1),ln_sample, type ='l', xlab = "time", ylab = "X")
```

```{r, fig.cap="\\label{fig:lognormal_hist} Histogram of generated samples with log-normal proposal (MH sampler)", out.width = "80%", fig.pos='h', fig.align='center' }

hist(ln_sample, breaks = 20, main = "", xlab = "X")
```


### 2.

Now we perform Step 1 using the chi-square distribution $\chi^2(\lfloor X_t+1\rfloor)$ as the proposal distribution, where $\lfloor x\rfloor$ is the \texttt{floor()} function.

```{r}

# 1.2
#Set seed
set.seed(1234567890)

posterior_sampler_chisq <- function(tmax=1000, starting_point = 1) {
  X=rep(starting_point,tmax)
  for (t in 1:tmax) {
    Y = rchisq(1,df = floor(X[t]+1))
    U = runif(1,min = 0, max = 1)
    alpha = min(1,
                (posterior(Y) * dchisq(X[t],df = floor(Y+1))) /
                (posterior(X[t]) * dchisq(Y, df = floor(X[t]+1))))
    if (U < alpha) {
      X[t+1] = Y
    } else {
      X[t+1] = X[t]
    }
    t = t+1
  }
  return(X)
}
```

The time series plot is shown in Figure \ref{fig:chisquare}, and it can be seen that the chain converges much more than in the previous case. Again there does not seem to be a burn-in period, the starting value of 1 seems to be a probable value of the distribution, so we do not discard any samples from the beginning of the chain.

```{r, fig.cap="\\label{fig:chisquare} Metropolis-Hastings sampler with chi-square proposal", out.width = "80%", fig.pos='h', fig.align='center' }
tmax=1000
# Seed
set.seed(1234567890)
chi_sample = posterior_sampler_chisq(tmax = tmax, starting_point = 1)
plot(1:(tmax+1), chi_sample, type = 'l', xlab = "time", ylab = "X")
```

```{r, fig.cap="\\label{fig:chisquare_hist} Histogram of generated samples with Chi-square proposal (MH sampler)", out.width = "80%", fig.pos='h', fig.align='center' }
hist(chi_sample, breaks = 20, main = "", xlab = "X")
```


### 3.

We recognise that the probability distribution function $f(x)$ belongs to a certain gamma distribution, therefore we know that $f(x)$ must have a positive skew. This attribute is also true for the distribution of samples from Step 2 (Figure \ref{fig:chisquare_hist}), but untrue for the distribution of samples from Step 1 (Figure \ref{fig:lognormal_hist}). Also, the chain with the log-normal proposal does not converge, but the chain with the Chi-square proposal does, so we conclude that the samples generated from Step 2 are a better approximation of samples from the posterior distribution.

### 4.

Now we generate 10 MCMC sequences using the sample generator from Step 2, and analyze their convergence using the Gelman-Rubin method. The starting points for the 10 sequences are 1,2,...,10.


```{r, include=FALSE}

#1.4
starting_points = seq(1,10)
MCMC_seqs = mcmc.list()
tmax=1000
# Seed
set.seed(1234567890)
for (k in starting_points) {
  MCMC_seqs[[k]] = as.mcmc(posterior_sampler_chisq(tmax = tmax, starting_point = k))
}

print(gelman.diag(MCMC_seqs))
```
The output of the Gelman-Rubin's convergence diagnostic for 1000 iterations gives the potential scale reduction factors with the corresponding point estimate and the upper confidence limit. The potential scale reduction factor is based on the comparison of within-chain and between-chain variances. For 1000 iterations, its point estimate is 1.01 and its upper confidence limit is 1.02, which is very close to 1. So according to this diagnostics, approximate convergence has been achieved at 1000 iterations. Even for only 100 iterations, both the point estimate and the upper limit are under 1.1.



### 5.

In this step we estimate

$$\int\limits_{0}^{\infty}xf(x)dx$$
using the samples generated from Steps 1 and 2.

Based on the slides from Lecture 4 we can use MCMC samples from $p(\theta|D)$ to obtain a point estimator by calculating the sample average: $$\theta^* = \int\theta p(\theta|D) \approx \frac{1}{n} \sum\limits_{i=1}^{n}\theta_i$$
```{r}

# 1.5
# Seed
set.seed(1234567890)
#using Step1
#mean(ln_sample) #large variance; 2.949404

#using Step2
#mean(chi_sample) #small variance; 5.636084
```

The sample average of the chain generated in Step 1 is **2.95** and in Step 2 is **5.63**.

If we do not set the seed before the sample generation, then we get highly variable averages from Step 1, but from Step 2 it is almost always between 5.5 and 6.5.

### 6.

According to Definition 4.5 in Mathematical Statistics with Applications by Wackerly et al., the integral in Step 5 is equal to the expected value of random variable X, where X can take values between 0 and $\infty$.

The generated distribution with probability density function $f(x)$ is a gamma distribution, so we can express $f(x)$ in its general form as
$$f(x) = \Bigg{[}\frac{1}{\Gamma(\alpha)\beta^\alpha}\Bigg{]}x^{\alpha-1}e^{-x/\beta},$$


where $0<x<\infty$. Thus $E(X) = \alpha\beta$. In our case $\alpha = 6, \beta=1$, so $E(X) = 6$.

This is the actual value of the integral in Step 5, so we conclude that our estimation using the samples generated in Step 2 with a Chi-square proposal distribution seems accurate, whereas the estimation from the samples of Step 1 with the log-normal proposal does not seem accurate at all.

----------

# Question 2: Gibbs sampling

### 1.

The aim of this task is to restore the expected concentration values, given the data \texttt{chemical.RData}. We import the data into R, containing the concentration of a certain chemical in a water sample, having the following variables:

* \texttt{X}: day of the measurment,
* \texttt{Y}: measured concentration of the chemical.

The code can be found in the Appendix. 


```{r}


############################## Question 2: Gibbs sampling ###############################

load(file = "chemical.RData") 
```

Figure \ref{fig:chemical} shows the the dependence of \texttt{Y} on \texttt{X}, where a linear relationship between \texttt{Y} and \texttt{X} can be observed. Therefore a linear regression with normally distributed residuals seems like a good approach. 

```{r, fig.cap="\\label{fig:chemical} Dependendence of measured concentration of the chemical on day of the measurment", out.width = "80%", fig.pos='h', fig.align='center' }
# 2.1

plot(X,Y, xlab = "Day of the measurment"
     , ylab = "Measured concentration of the chemical", col = "grey", pch = 16)
```



### 2.

Using the random-walk Bayesian model $Y_{i} \sim N(\mu_{i}, \sigma^2=0.2)$ for $i=1,...,n$, with the prior $p(\mu_1)=1$, $p(\mu_{i+1}|\mu_i)=N(\mu_i,\sigma^2=0.2)$, for $i=1,...,{n-1}$. We denote the the number of observations with $n$ and we consider $\vec{\mu}=(\mu_{1},...,\mu_{n})$ as unknown parameters.  

Given this information we are able to compute the likelihood $p(\vec{Y}|\vec{\mu})$. Under the assumption that $Y_i$ are iid, the likelihood is given by

\begin{eqnarray*}
p(\vec{Y}|\vec{\mu}) &=& \prod_{i=1}^{n}f(Y_{i}|\mu_{i},\sigma^2) \\
                     &=& \prod_{i=1}^{n}\frac{1}{\sigma\sqrt{2\pi}} \ \text{exp}\left[-\frac{(Y_{i}-\mu_{i})^2}{2\sigma^{2}}\right]\\
                     &=& \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n \text{exp}\left[-\frac{\sum_{i=1}^{n}(Y_{i}-\mu_{i})^2}{2\sigma^{2}}\right].\\
\end{eqnarray*}


Further, we will compute the prior by using the chain rule $p(\vec{\mu})=p(\mu_{1})p(\mu_{2}|\mu_{1})...p(\mu_{n}|\mu_{n-1})$. Then the prior is given by
\begin{eqnarray*}
p(\vec{\mu}) &=& p(\mu_1)\prod_{i=1}^{n-1}p(\mu_{i+1}|\mu_{i}) \\
             &=& \prod_{i=2}^{n}\frac{1}{\sigma\sqrt{2\pi}} \ \text{exp}\left[-\frac{(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]\\
             &=& \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n \text{exp} \left[-\frac{\sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]. \\
\end{eqnarray*}

### 3.

In order to get the posterior, up to a constant proportionality, we use the Bayes' Theorem. Therefore, 

\begin{eqnarray*}
p(\vec{\mu}|\vec{Y}) &\propto& p(\vec{Y}|\vec{\mu})p(\vec{\mu})\\
                     &\propto& \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n \text{exp}\left[-\frac{\sum_{i=1}^{n}(Y_{i}-\mu_{i})^2}{2\sigma^{2}}\right] \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n \text{exp}\left[-\frac{\sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]\\
                     &\propto& \text{exp}\left[-\frac{\sum_{i=1}^{n}(Y_{i}-\mu_{i})^2}{2\sigma^{2}}\right] \text{exp}\left[-\frac{\sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]\\
                     &\propto& \text{exp}\left[-\frac{\sum_{i=1}^{n}(Y_{i}-\mu_{i})^2 + \sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right].
\end{eqnarray*}

And finally, once we have the posterior, we can compute the the probability distribution $p(\mu_{i}|\vec{\mu}_{i},\vec{Y})$, where $\vec\mu_{-i}$ is a vector containing all $\mu$ values except for $\mu_{i}$. We also know that we can discard all the elements that do not contain $\vec\mu_{i}$, as we are looking for a posterior up to a constant proportionality. Given Hint A, we will consider separate formulas for $p(\mu_{1}|\vec{\mu}_{1},\vec{Y})$ and for $p(\mu_{n}|\vec{\mu}_{n},\vec{Y})$. By using Hint B, we obtain

$$p(\mu_{1}|\vec{\mu}_{-1},\vec{Y}) \propto \text{exp}\left[-\frac{\left(\mu_{1}-(Y_{1}+\mu_{2})/2\right)^{2}}{\sigma^{2}}\right], \quad \text{for }i=1,$$

and


$$p(\mu_{n}|\vec{\mu}_{-1},\vec{Y}) \propto \text{exp}\left[-\frac{\left(\mu_{n}-(Y_{n}+\mu_{n-1})/2\right)^{2}}{\sigma^{2}}\right], \quad \text{for }i=n.$$



By using Hint C, we get to the result

$$p(\mu_{i}|\vec{\mu}_{-i},\vec{Y}) \propto \text{exp}\left[-\frac{\left(\mu_{i}-(Y_{i}+\mu_{i-1}+\mu_{i+1})/3\right)^{2}}{2\sigma^{2}/3}\right], \quad \text{for } i=2,...,n-1.$$


### 4. 

We implement the Gibbs samples using the distributions found and initial value $\vec\mu^{0}=(0,...,0)$.  This code snippet (see Appendix) computes $n$ values of $\vec\mu$ and computes the expected value of $\vec\mu$ using a Monte Carlo approach. In this particular case $n=1000$. 

The expected value of $\vec\mu$ is: 

```{r}
# 2.4

gibbs_sampler <- function(Tmax) {
  d <- length(Y)
  sigma <- 0.2
  mu <- matrix(0, nrow = Tmax, ncol = d)
  t <- 1
  while (t<Tmax) {
    for (i in 1:d) {
      if (i==1) {
        mu[t+1,i] <- exp((-(mu[t,i]-(Y[i] + mu[t,i+1])/2)^2)/(sigma^2))
      }
      else if (i==d) {
        mu[t+1,i] <- exp((-(mu[t,i]-(Y[i] + mu[t+1,i-1])/2)^2)/(sigma^2))
      }
      else {
        mu[t+1,i] <- exp((-3*(mu[t,i]-(Y[i]+mu[t+1,i-1]+mu[t,i+1])/3)^2)/(2*(sigma^2)))
      }
    }
  t=t+1
  }
  mu
}

Y_gibbs <- colSums(gibbs_sampler(1000))/1000
```

In \ref{fig:gibbs} there is a plot of the dependence of \texttt{Y} on \texttt{X}.  

```{r, fig.cap="\\label{fig:gibbs} Comparison between Y and expected value of mu", out.width = "80%", fig.pos='h', fig.align='center' }
# 2.1

plot(X,Y, xlab = "Day of the measurment"
     , ylab = "Measured concentration of the chemical", col = "grey", pch = 16)
points(Y_gibbs, col = "red", pch = 16)
```

It makes sense that the concentration decresases with time. The burn out period is of 15 days, after that the concentration is about 0. 




----------

# Appendix

```{r ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

```