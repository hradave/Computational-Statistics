---
title: '732A90: Computational Statistics'
author: "Sofie Jörgensen, Oriol Garrobé Guilera, David Hrabovszki"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document:
    fig_caption: yes
    includes:
      in_header: my_header.tex
subtitle: Computer lab4 - Group11
header-includes: \usepackage{float}
---

```{r setup, include=FALSE, eval=TRUE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Question 2:

### 1.

We import into R the data containing the concentration of a certain chemical in a water sample is found in \texttt{chemical.RData}, having the following variables:
 -\texttt{X}: day of the measurment
 -\texttt{Y}: measured concentration of the chemical
The code can be found in the Appendix. 

```{r}
############################## Question 2: Gibbs sampling ###############################

load(file = "chemical.RData") 
```

In \ref{fig:chemical} there is a plot of the dependence of \texttt{Y} on \texttt{X}.  

```{r, fig.cap="\\label{fig:chemical} Dependendence of measured concentration of the chemical on day of the measurment", out.width = "80%", fig.pos='h', fig.align='center' }
# 2.1

plot(X,Y, xlab = "Day of the measurment", ylab = "Measured concentration of the chemical", col = "grey", pch = 16)
```

### 2.

Using the random-walk Bayesian model with $n$ number of observations and $\vec{\mu}=(\mu_{1},...,\mu_{n})$ uknown parameters: $$Y_{i}=N(\mu_{i}, 0.2),\quad i=1,...,n$$

We compute the likelihood $p(\vec{Y}|\vec{\mu})$ following the steps below:

\begin{eqnarray*}
p(\vec{Y}|\vec{\mu}) &=& \prod_{j=1}^{n}f(Y_{j}|\mu_{j},0.2) \\
                     &=& \prod_{j=1}^{n}\frac{1}{\sigma\sqrt{2\pi}}exp\left[-\frac{(Y_{j}-\mu_{j})^2}{2\sigma^{2}}\right]\\
                     &=& \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n exp\left[-\frac{\sum_{j=1}^{n}(Y_{j}-\mu_{j})^2}{2\sigma^{2}}\right]\\
\end{eqnarray*}

In order to compute the prior $p(\mu_{i})$ a chain rule is used. Being the chain rule:$p(\vec{\mu})=p(\mu_{1})p(\mu_{2}|\mu_{1})...p(\mu_{n}|\mu_{n-1})$ 

Let the prior be, 

\begin{equation}
p(\mu_{1})=1\\
p(\mu_{i+1|\mu_{i}})=N(\mu_{i}, 0.2)
\end{equation}

We follow the steps below:

\begin{eqnarray*}
p(\vec{\mu}) &=& \prod_{i=1}^{n}p(\mu_{1})p(\mu_{i+1}|\mu_{i}) \\
             &=& \prod_{i=2}^{n}N(\mu_{i},0.2)\\
             &=& \prod_{i=2}^{n}\frac{1}{\sigma\sqrt{2\pi}}exp\left[-\frac{(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]\\
             &=& \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n exp\left[-\frac{\sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]\\
\end{eqnarray*}

### 3.

In order to get the posterior up to a constant proportionality, we use the Bayes'. Therefore, 

\begin{eqnarray*}
p(\vec{\mu}|\vec{Y}) &\propto& p(\vec{Y}|\vec{\mu})p(\vec{\mu})\\
                     &\propto& \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n exp\left[-\frac{\sum_{j=1}^{n}(Y_{j}-\mu_{j})^2}{2\sigma^{2}}\right] \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n exp\left[-\frac{\sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]\\
                     &\propto& exp\left[-\frac{\sum_{j=1}^{n}(Y_{j}-\mu_{j})^2}{2\sigma^{2}}\right] exp\left[-\frac{\sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]\\
                     &\propto& exp\left[-\frac{\sum_{j=1}^{n}(Y_{j}-\mu_{j})^2 + \sum_{i=2}^{n}(\mu_{i}-\mu_{i-1})^2}{2\sigma^{2}}\right]

                     
\end{eqnarray*}

And finally, once we have the posterior, we can compute the the probability distribution $p(\mu_{i}|\vec{\mu}_{i},\vec{Y})$, where $\vec\mu_{-i}$ is a vector containing all $\mu$ values except for $\mu_{i}$.We also know that we can discard all the elements that do not contain $\vec\mu_{i}$ as we are looking for a posterior up to a constant proportionality. Using Hint C, we get to the following result:

$$p(\mu_{i}|\vec{\mu}_{-i},\vec{Y}) \propto exp\left[-\frac{\left(\mu_{i}-(Y_{i}+Y_{i-1}+Y_{i+1})/3\right)^{2}}{2\sigma^{2}/3}\right]$$

### 4. 

We implement the Gibbs samples using the distributions found and initial value $\vec\mu^{0}=(0,...,0)$.  This code snippet (see Appendix) computes $n$ values of $\vec\mu$ and computes the expected value of $\vec\mu$ using a Monte Carlo approach. In this particular case $n=1000$. 

The expected value of $\vec\mu$ is: 

###I'M NOT SURE ABOUT THIS
```{r}
# 2.4

gibbs_sampler <- function(Tmax) {
  d <- length(Y)
  sigma <- 0.2
  mu <- matrix(0, nrow = Tmax, ncol = d)
  t <- 0
  while (t<Tmax) {
    for (i in 1:d) {
      mu[t+1,i] <- exp((-3*(mu[i]-(Y[i]+Y[i+1])/3)^2)/(2*sigma^2))
    }
  }
  mu
}

gibbs_sampler(10)
```


