---
title: '732A90: Computational Statistics'
author: "Sofie Jörgensen, Oriol Garrobé Guilera, David Hrabovszki"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    fig_caption: yes
    includes:
      in_header: my_header.tex
  html_document:
    df_print: paged
subtitle: Computer lab6 - Group11
header-includes: \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r warning = FALSE}
# R version
RNGversion('3.5.1')
library("ggplot2")
```

# Question 1: Genetic algorithm

In this exercise we are going to perform one-dimensional maximization by using a genetic algorithm.

### 1.

Firstly, we define the function \texttt{f()} as

$$
f(x):= \frac{x^2}{e^x} -2 \ \text{exp} (-(9\text{sin } x)/(x^2+x+1)). 
$$

```{r}
#1.1
f <- function(x){
  return(x^2/exp(x) - 2*exp(-1*(9*sin(x)) / (x^2 + x + 1)))
}

```


### 2.

Secondly, we define the function \texttt{crossover()}, that takes two scalars $x$ and $y$ as inputs, and returns a child as $\frac{x+y}{2}$.

```{r}
#1.2
crossover <- function(x,y){
  return((x+y) / 2)
}

```

### 3.

Thirdly, we define the function \texttt{mutate()}, that performs the integer division $x^2 \text{  mod } 30$, for a scalar input $x$. 

```{r}
#1.3
mutate <- function(x){
  return(x^2 %% 30)
}
```


### 4.


Further, we will create a function called \texttt{genetic()}, with the parameters \texttt{maxiter} and \texttt{mutprob}. The settings of this \texttt{genetic()} function, as well as its output results, are presented in (a)-(e). The code can be found in the Appendix.


(a). The function $f()$ is plotted in the range from 0 to 30 in Figure X, and we can observe that there is a maximum value located around $x=1$.

(b). An initial population for the genetic algorithm is defined as $X = (0, 5, 10, 15,... ,30)$.

(c). A vector called \texttt{Values} are computed, containing the function values for each population point.


(d). The \texttt{genetic()} function performs \texttt{maxiter} iterations. For each iteration...

(e).

```{r}
#4
#4
genetic <- function(maxiter, mutprob){
  #a
  plot(x = seq(0,30), y = f(seq(0,30)), type = "l", xlab = "x", ylab = "f(x)")
  abline(v=seq(0,30)[which.max(f(seq(0,30)))], col="red" )
  
  #b
  X = seq(0,30,5)
  
  #c
  Values = f(X)
  
  #d
  #set seed
  set.seed(1234567890)
  for (i in 1:maxiter) {
    #i
    parents = match(sample(X, 2),X)
    
    #ii
    victim = order(Values)[1]
    
    #iii
    kid = round(crossover(parents[1],parents[2]))
    p = runif(1)
    if (p < mutprob) {
      kid = mutate(kid)
    }
    
    #iv
    X[victim] = kid
    Values = f(X)
    
    #v
    max = max(Values)
  }
  
  #e
  print(X)
  print(Values)
  plot(x = seq(0,30), y = f(seq(0,30)), type = "l", xlab = "x", ylab = "f(x)")
  points(x = X, y = Values, col = "red", pch = 19)
}


```





### 5.

By using the defined functions from previous tasks (1.1-1.4), we are going to observe the initial population and final population. This is done by running the code with different combinations of \texttt{maxiter}$= 10, 100$ and \texttt{mutprob}$= 0.1, 0.5, 0.9$.


```{r}
# Just testing no change, i.e. initial population
genetic(1,0)
```




# Question 2: EM algorithm

The purpose with this exercise is to implement the EM algorithm. For this, we are given the data file \texttt{physical1.csv}, containing a behavior of two related physical processes $Y = Y(X)$ and $Z = Z(X)$.

### 1.

The first step is to examine the data set \texttt{physical1.csv}, to see if the two processes are related to each other. 

```{r}
# 1.1 
physical <- read.csv2("physical1.csv", sep = ",")

X <- as.numeric(as.character(physical$X))
Y <- as.numeric(as.character(physical$Y))
Z <- as.numeric(as.character(physical$Z))

data <- data.frame(X = c(X,X), value = c(Y,Z), Process= rep(c("Y","Z"), each= 100))

# var(Z,na.rm = TRUE)
# var(Y,na.rm = TRUE)

```

```{r, fig.cap="\\label{fig:timeseries} Time series plot of the dependence of Z and Y versus X.", out.width = "80%", fig.pos='!h', fig.align='center'}
# Time series plot 
ggplot(data = data, aes(x = X, y = value, col = Process)) + 
  geom_line() + 
  ylab("Physical process") + 
  theme_bw()

```

In Figure \ref{fig:timeseries} it seems that the two processes are related to each other, with respect to $X$, since the graphs follows similar patterns. We can also observe that the physical process $Z$ has a greater variation, especially at the beginning of the series, but also in general, compared to the process $Y$. 




### 2.

Using the following model,

\begin{eqnarray*}
Y_{i} \stackrel{}\sim exp\left(\frac{X_{i}}{\lambda}\right) \\
Z_{i} \stackrel{}\sim exp\left(\frac{X_{i}}{2\lambda}\right)
\end{eqnarray*}

where $\lambda$ is an unknown parameter, we derive the EM algorithm to estimate $\lambda$. The code can be found in the Appendix.

```{r}

EM_algorithm <- function(X, Z, lambda, eps, k_max) {
    Z_obs <- Z[!is.na(Z)]
    Z_miss <- Z[is.na(Z)]
    
    X_obs <- X[!is.na(Z)]
    X_miss <- X[is.na(Z)]
    
    n <- length(c(Z_obs, Z_miss))
    r <- length(Z_obs)

    k<-1
    lambda_prev <- lambda
    lambda_curr <- lambda+10+100*eps #random number to initialize the algorithm
    
    while (k<k_max+1 && abs(lambda_prev-lambda_curr)>=eps) {
      
	    lambda_prev<-lambda_curr
	    
	    ## E-step
	    EY <- -n*log(2*lambda_prev) + sum(log(X)) - sum(X_obs*Z_obs)/(2*lambda_prev) - (n-r)*lambda_curr/lambda_prev
	    
	    ## M-step
      lambda_curr <- (sum(X_obs*Z_obs)+2*(n-r)*lambda_prev)/n
	    
	    k<-k+1
	    
    }
    return(c(number_of_iterations = k-1, optimal_lambda = lambda_curr))
}
```


### 3.
```{r}

EM_algorithm(X,Z, 100, 0.001, 100)
```





### 4.








```{r warning=FALSE}
# R version
RNGversion('3.5.1')
# Packages

```



----------

# Appendix

```{r ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

```